{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "if str(os.getcwdb()[-3:]).split(\"'\")[1] != 'src':\n",
    "    os.chdir(os.path.dirname(os.getcwdb()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bazu = plt.imread(r'images\\bazu.jpg')\n",
    "\n",
    "bazu.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: mobilenet_v2_140_224 : https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mobilenet_v2_140_224\"\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "# Tratamiento de imágenes\n",
    "import cv2\n",
    "import splitfolders\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "from utils.funciones import define_x_y, definir_modelo\n",
    "\n",
    "\n",
    "# 1. Procesamiento de datos\n",
    "# ================================================================================================================\n",
    "\n",
    "# Es necesario definir el directorio de trabajo\n",
    "os.chdir(str(input('Introduce directorio de trabajo:')))\n",
    "\n",
    "# Dividimos los datos en train y test\n",
    "\n",
    "if 'train' not in os.listdir(r'data/processed') and 'val' not in os.listdir(r'data/processed'): # Si no se ha ejecutado ya, entonces se ejecutará\n",
    "    print('Para guardar el modelo, es necesario que los datos de las imágenes estén, dentro de su directorio del proyecto, en \"data/raw\". Si no lo tiene así guardado, por favor, descárgelos de https://www.kaggle.com/datasets/smaranjitghose/corn-or-maize-leaf-disease-dataset y guárdelos en \"data/raw\"')\n",
    "    path = r'data/raw' # Directorio donde están ubicadas las imágenes sin dividir en train /test /val\n",
    "    splitfolders.ratio(path, output=\"data/processed\", seed=42, ratio=(.8, .1, .1), move=True) # Así se divide en train /test /val y se guardan en el directoriio indicado\n",
    "    print('Las imágenes descargadas se han movido a \"data/processed\" y se han separado en train / val y / test respectivamente')\n",
    "\n",
    "# Definimos los directorios donde se encuentran las imágenes\n",
    "path_train = r'data/processed/train'\n",
    "path_val = r'data/processed/val'\n",
    "path_test = r'data/processed/test'\n",
    "tipos = os.listdir(path_train) # Categorías a clasificar\n",
    "\n",
    "\n",
    "# Tranformaciones\n",
    "\n",
    "\n",
    "\n",
    "# Definimos el path del directorio y creamos dataset\n",
    "X_train, y_train = define_x_y(path_train)\n",
    "X_val, y_val = define_x_y(path_val)\n",
    "X_test, y_test = define_x_y(path_test)\n",
    "\n",
    "# Damos un valor numérico a cada categoría\n",
    "target_dict_train ={k: v for v, k in enumerate(np.unique(y_train))}\n",
    "target_dict_val ={k: v for v, k in enumerate(np.unique(y_val))}\n",
    "target_dict_test ={k: v for v, k in enumerate(np.unique(y_test))}\n",
    "\n",
    "if type(y_train) == list: # Si esta celda no se ha ejectuado aún, entonces se ejecutará\n",
    "\n",
    "    # Transformamos las variablse target \"y_train\", \"y_val\" e \"y_test\" en numéricas \n",
    "    y_train =  [target_dict_train[y_train[i]] for i in range(len(y_train))]\n",
    "    y_val =  [target_dict_val[y_val[i]] for i in range(len(y_val))]\n",
    "    y_test =  [target_dict_test[y_test[i]] for i in range(len(y_test))]\n",
    "\n",
    "    # Transformamos tanto \"X\" como \"y\" en numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    X_val = np.array(X_val)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "    y_val = np.asarray(y_val).astype('float32').reshape((-1,1))\n",
    "    y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "\n",
    "\n",
    "\n",
    "# 2. Definición del modelo\n",
    "# ================================================================================================================\n",
    "\n",
    "# Escogemos la red neuronal base:\n",
    "base_model = 'https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5' \n",
    "\n",
    "model = definir_modelo(base_model)\n",
    "\n",
    "print('Aplicándose transfer learning')\n",
    "\n",
    "\n",
    "\n",
    "# 3. Entrenamiento del modelo\n",
    "# ================================================================================================================\n",
    "\n",
    "# Para prevenir overfitting, establecemos un número alto de 'epoch'\n",
    "print(\"Su modelo se está entrenando. Vaya a por un café o tómese un descanso, y vuelva en unos minutos\")\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', # Ante datos nuevos (distintos del test), parará\n",
    "                                mode='min', patience=5,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "                    epochs=100, # el número de veces que se reentrena el modelo\n",
    "                    batch_size=128, # El número de samples que se procesan antes de que el modelo se actualice\n",
    "                    verbose=2, \n",
    "                    callbacks=[earlystopping], # Parará en el 'epoch' óptimo\n",
    "                    validation_data=(X_val, y_val) \n",
    "                    )\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# 4. Guardado del modelo\n",
    "# ================================================================================================================\n",
    "\n",
    "# Vamos a guardar nuestro modelo\n",
    "if 'my_model' not in os.listdir(r'model'): # Si no se ha ejecutado esta celda ya, entonces se ejecutará\n",
    "    model.save(\"my_model\")\n",
    "    print('su modelo ha sido guardado con éxito')\n",
    "\n",
    "else:\n",
    "    print('su modelo ya ha sido guardado previamente')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62be4b4d7aada9f05487a097e316e83dc3ceda15568e9d0ea281b513767b88d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
